[
  {
    "path": "posts/welcome/",
    "title": "Welcome to R for Data Science and Analytics",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "TAng Yue",
        "url": "https://scis.smu.edu.sg/"
      }
    ],
    "date": "2021-07-26",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-26T16:30:44+08:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-07-26-assignmentvastchallenge2021/",
    "title": "assignment_VAST_Challenge_2021",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "TAng Yue",
        "url": "https://scis.smu.edu.sg/"
      }
    ],
    "date": "2021-07-26",
    "categories": [],
    "contents": "\r\n1. Overview\r\nThe 2021 Visual Analytics Science and Technology (VAST) Challenge presented researchers with a single fictitious scenario: the disappearance of staff members of the GASTech oil and gas company on location on the island of Kronos. A group named the Protectors of Kronos (POK) was the prime suspect in the disappearance. Three mini-challenges and a grand challenge were offered. For more information, please see VAST Challenge 2021.\r\nThis module will research Mini-Challenge 3 which includes multiple types of text data for participants to feature real-time streaming social media and emergency service data for participants to provide hostage and kidnapper information.This challenge has 3 tasks and questions and asked the participants to integrate results to evaluate the changing levels of risk to the public and recommend actions.\r\n2. Data preparation and Exploration\r\n2.1 Data Source\r\nThere are three dataset provides in Mini-Challenge 3 :\r\nMicroblog records that have been identified by automated filters as being potentially relevant to the ongoing incident\r\nText transcripts of emergency dispatches by the Abila, Kronos local police and fire departments.\r\nmaps of Abila and background documents.\r\n\r\nThe data of Microblog and text transcripts of emergency dispatches are found in three segments:\r\nSegment 1 :“csv-1700-1830.csv” - covers the time period from 1700 to 1830 Abila time on January 23.\r\nSegment 2: \"csv-1831-2000.csv’- covers the time period from 1830 to 2000 Abila time on January 23.\r\nSegment 3: “csv-2001-2131.csv” - covers the time period from 2000 to shortly after 2130 Abila time on January 23.\r\n\r\n2.2 Install and load R package\r\nIn this module, the tidyverse, tidytext, dplyr and other packages will be used, which could be seen from below code chunk.\r\n\r\n\r\npackages = c('tidytext', 'tidyverse','dplyr','tm',\r\n             'widyr', 'wordcloud','lubridate','wordcloud',\r\n             'DT', 'ggwordcloud', 'SnowballC','ggplot2',\r\n             'tokenizers', 'lubridate', 'topicmodels','stringr',\r\n             'hms','tidyverse', 'stringr','clock','tmap',\r\n             'tidygraph', 'ggraph', 'tidytext','dygraphs','sf',\r\n             'igraph','rgdal','raster','sp','patchwork','hrbrthemes')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\n2.3 Import dataset and combine data\r\nThe data of Microblog records and emergency calls are stored in separate csv files. And three csv files share same columns but data generate from different date, which is shown below.\r\n\r\nFirstly, we need to combine 3 files into one consolidated file, which is necessary for the following analysis. In this step, package tidyverse would be used.The following R code shows the process of data consolidation, then three datasets with different date have been integrated into one file.\r\n\r\n\r\nlibrary(tidyverse)\r\ntable1 <- read.csv(\"data/csv-1700-1830.csv\")\r\ntable2 <- read.csv(\"data/csv-1831-2000.csv\")\r\ntable3 <- read.csv(\"data/csv-2001-2131.csv\")\r\ndata <- rbind(table1, table2,table3)\r\n\r\n\r\n\r\n2.4 Modifying Date format\r\nconverting character objects to dates can be made easier by using the lubridate package, which would be used to convert date type from ‘yyyymmddhhmmss’ to ‘yyyy-mm-dd hh:mm:ss’, and create a new column ‘date’ in data, and the code chunk could be seen below.\r\n\r\n\r\ndata$date.yyyyMMddHHmmss. <- ymd_hms(data$date.yyyyMMddHHmmss.)\r\n\r\n\r\n\r\nRename “date.yyyyMMddHHmmss.” in data into “date” with concise format.\r\n\r\n\r\nnames(data)[names(data) == \"date.yyyyMMddHHmmss.\"] <- \"date\"\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-26T16:47:21+08:00",
    "input_file": "assignmentvastchallenge2021.knit.md"
  }
]
